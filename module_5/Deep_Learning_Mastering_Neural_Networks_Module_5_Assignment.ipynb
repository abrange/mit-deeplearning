{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrange/mit-deeplearning/blob/main/module_5/Deep_Learning_Mastering_Neural_Networks_Module_5_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZVJ8gUn1RR9"
      },
      "source": [
        "# Deep Learning: Mastering Neural Networks - Module 5 Assignment: LSTM Sentence Completion\n",
        "\n",
        "Now that we have a framework for working with sequential data in PyTorch - we would like to improve our sentence completion model by introducing a more sophisticated dataset encoding and neural network architecture.\n",
        "\n",
        "In this assignment, we would like you to implement an LSTM model that contains 2 hidden layers and completes sentences at a word level encoding instead of character. We will provide code for cleaning and preparing the data as well as some helper functions so that you can complete the task.\n",
        "\n",
        "Note: This LSTM can take a long time to train. Try using a small number of epochs or a small dataset(~10 samples) to verify your network can train properly before using the full dataset and a larger number of Epochs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4mucQioGZA"
      },
      "source": [
        "## Dataset and Encoding\n",
        "\n",
        "We will use the same dataset as the last notebook, however we will now use the spanish sentences as the targets for our sequence!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrzZRul42zJs"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import time, copy\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoRgk40z3kJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3acf79a-7a23-477b-fba6-288d3ccab719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'spa-eng.zip': No such file or directory\n",
            "rm: cannot remove '_about.txt': No such file or directory\n",
            "rm: cannot remove 'spa.txt': No such file or directory\n",
            "--2025-07-28 07:56:05--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5453910 (5.2M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.20M  3.27MB/s    in 1.6s    \n",
            "\n",
            "2025-07-28 07:56:08 (3.27 MB/s) - ‘spa-eng.zip’ saved [5453910/5453910]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n",
            "_about.txt  sample_data  spa-eng.zip  spa.txt\n"
          ]
        }
      ],
      "source": [
        "# Here we download and unzip the text file that contains all of our translated phrases\n",
        "!rm spa-eng.zip _about.txt spa.txt\n",
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66lR-Qyl8BYs"
      },
      "outputs": [],
      "source": [
        "# Helper functions combined from PyTorch tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "# This is important because we want all words to be formatted the same similar\n",
        "# to our image normalization\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\"\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!'?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def parse_data(filename):\n",
        "    # Read the file and split into lines\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    # Throw out the attribution as it is not a part of the data\n",
        "    pairs = [[pair[0], pair[1]] for pair in pairs]\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti0WFHKX9-k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca627efd-6bda-4f77-a9b5-5ba4ee3bb37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of English sentences: 142511\n"
          ]
        }
      ],
      "source": [
        "pairs = parse_data(\"spa.txt\")\n",
        "# We only want the english sentences because we aren't translating\n",
        "english_sentences = [pair[0] for pair in pairs]\n",
        "# Shuffle our dataset\n",
        "random.shuffle(english_sentences)\n",
        "print(\"Number of English sentences:\", len(english_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BngUGRRE-8P0"
      },
      "outputs": [],
      "source": [
        "# Since we already shuffled our dataset, grab a random sampling of sentences for our train, val, and test\n",
        "# Here we are using a small number of Sentences to ease training time. Feel free to use more\n",
        "train_sentences = english_sentences[:20000]\n",
        "val_sentences = english_sentences[20000:30000]\n",
        "test_sentences = english_sentences[3000:40000]\n",
        "\n",
        "# Using this function we will create a dictionary to use for our one hot encoding vectors\n",
        "def add_words_to_dict(word_dictionary, word_list, sentences):\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split(\" \"):\n",
        "            if word in word_dictionary:\n",
        "                continue\n",
        "            else:\n",
        "                word_list.append(word)\n",
        "                word_dictionary[word] = len(word_list)-1\n",
        "\n",
        "english_dictionary = {}\n",
        "english_list = []\n",
        "add_words_to_dict(english_dictionary, english_list, train_sentences)\n",
        "add_words_to_dict(english_dictionary, english_list, val_sentences)\n",
        "add_words_to_dict(english_dictionary, english_list, test_sentences)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pszv87RStebQ"
      },
      "source": [
        "### Encoding\n",
        "\n",
        "We will encode our sequences in a very similar format to the previous tasks. However, our one-hot encoding vectors will encode over a dictionary of words instead of specific characters. This will result in a larger one hot encoding vector but a shorter overall sequence length for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MLDsB9KSYvQ"
      },
      "outputs": [],
      "source": [
        "# Now make our training samples:\n",
        "def create_input_tensor(sentence, word_dictionary):\n",
        "    words = sentence.split(\" \")\n",
        "    tensor = torch.zeros(len(words), 1, len(word_dictionary)+1)\n",
        "    for idx in range(len(words)):\n",
        "        word = words[idx]\n",
        "        tensor[idx][0][word_dictionary[word]] = 1\n",
        "    return tensor\n",
        "\n",
        "def create_target_tensor(sentence, word_dictionary):\n",
        "    words = sentence.split(\" \")\n",
        "    tensor = torch.zeros(len(words), 1, len(word_dictionary)+1)\n",
        "    for idx in range(1, len(words)):\n",
        "        word = words[idx]\n",
        "        if word not in word_dictionary:\n",
        "            print(\"Error: This word is not in our dataset - using a zeros tensor\")\n",
        "            continue\n",
        "        tensor[idx-1][0][word_dictionary[word]] = 1\n",
        "    tensor[len(words)-1][0][len(word_dictionary)] = 1 # EOS\n",
        "    return tensor\n",
        "\n",
        "\n",
        "train_tensors = [(create_input_tensor(sentence, english_dictionary), create_target_tensor(sentence, english_dictionary)) for sentence in train_sentences]\n",
        "val_tensors = [(create_input_tensor(sentence, english_dictionary), create_target_tensor(sentence, english_dictionary)) for sentence in val_sentences]\n",
        "test_tensors = [(create_input_tensor(sentence, english_dictionary), create_target_tensor(sentence, english_dictionary)) for sentence in test_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnIg_qVDQhy0"
      },
      "outputs": [],
      "source": [
        "def tensor_to_sentence(word_list, tensor):\n",
        "    sentence = \"\"\n",
        "    for i in range(tensor.size(0)):\n",
        "        topv, topi = tensor[i].topk(1)\n",
        "        if topi[0][0] == len(word_list):\n",
        "            sentence += \"<EOS>\"\n",
        "            break\n",
        "        sentence += word_list[topi[0][0]]\n",
        "        sentence += \" \"\n",
        "    return sentence\n",
        "\n",
        "print(\"This code helps visualize which words represent an input_tensor and its corresponding target_tensor!\")\n",
        "examples_to_show = 6\n",
        "count = 1\n",
        "for input, target in train_tensors:\n",
        "    print(tensor_to_sentence(english_list, input))\n",
        "    print(tensor_to_sentence(english_list, target))\n",
        "    count +=1\n",
        "    if count > examples_to_show:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohr0XRAnl1Xx"
      },
      "outputs": [],
      "source": [
        "# Let's look at a few sentence encodings, to see what those look like:\n",
        "for i in range(3):\n",
        "    print(train_sentences[i], \"[encode as]\", train_tensors[i][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SjmOheE6bcn"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'train': train_tensors,\n",
        "               'val': val_tensors,\n",
        "               'test': test_tensors}\n",
        "\n",
        "dataset_sizes = {'train': len(train_tensors),\n",
        "                 'val': len(val_tensors),\n",
        "                 'test': len(test_tensors)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKtltT2-oUuo"
      },
      "source": [
        "### LSTM Definition\n",
        "\n",
        "Fill in your model in this section - a skeleton has been provided!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYLj4Qvzxldp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        # Save sizes\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Two-layer LSTM implemented with LSTMCell so we can feed one time step at a time\n",
        "        self.lstm1 = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.lstm2 = nn.LSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "        # Final classifier from hidden state to vocabulary logits\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        input:  Tensor of shape (1, input_size)  [one-hot for a single word]\n",
        "        hidden: tuple(h, c) where each is Tensor of shape (2, hidden_size)\n",
        "                index 0 = layer1, index 1 = layer2\n",
        "        returns: (logits, new_hidden)\n",
        "        \"\"\"\n",
        "        h_all, c_all = hidden  # each (2, hidden_size)\n",
        "        # Add batch dimension expected by LSTMCell: (1, hidden_size)\n",
        "        h1, h2 = h_all[0].unsqueeze(0), h_all[1].unsqueeze(0)\n",
        "        c1, c2 = c_all[0].unsqueeze(0), c_all[1].unsqueeze(0)\n",
        "\n",
        "        # Layer 1\n",
        "        h1, c1 = self.lstm1(input, (h1, c1))\n",
        "        # Layer 2\n",
        "        h2, c2 = self.lstm2(h1, (h2, c2))\n",
        "\n",
        "        # Project to vocabulary space\n",
        "        logits = self.out(h2)  # shape: (1, output_size)\n",
        "\n",
        "        # Return hidden state in the same (2, hidden_size) format the notebook uses\n",
        "        new_h = torch.stack([h1.squeeze(0), h2.squeeze(0)], dim=0)\n",
        "        new_c = torch.stack([c1.squeeze(0), c2.squeeze(0)], dim=0)\n",
        "        return logits, (new_h, new_c)\n",
        "\n",
        "    def initHidden(self):\n",
        "        # We need two hidden layers because of our two layered lstm!\n",
        "        # Your model should be able to use this implementation of initHidden()\n",
        "        return (torch.zeros(2, self.hidden_size).to(device), torch.zeros(2, self.hidden_size).to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoQcZxUF-QBY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e07aad89-7256-4d0e-e991-5e448e649f40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.LSTM"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>LSTM</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing them to be nested in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will also have their\n",
              "parameters converted when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRR_nA2ofl8t"
      },
      "outputs": [],
      "source": [
        "def train_lstm(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
        "    best_loss = np.inf\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Each epoch has a training, validation, and test phase\n",
        "    phases = ['train', 'val', 'test']\n",
        "\n",
        "    # Keep track of how loss evolves during training\n",
        "    training_curves = {}\n",
        "    for phase in phases:\n",
        "        training_curves[phase+'_loss'] = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data\n",
        "            for input_sequence, target_sequence in dataloaders[phase]:\n",
        "                # Now Iterate through each sequence here:\n",
        "\n",
        "                hidden = model.initHidden() # Start with a fresh hidden state\n",
        "\n",
        "                current_input_sequence = input_sequence.to(device)\n",
        "                current_target_sequence = target_sequence.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    loss = 0\n",
        "                    # Make a prediction for each element in the sequence,\n",
        "                    # keeping track of the hidden state along the way\n",
        "                    for i in range(current_input_sequence.size(0)):\n",
        "                        # Need to be clever with how we transfer our hidden layers to the device\n",
        "                        current_hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "                        output, hidden = model(current_input_sequence[i], current_hidden)\n",
        "                        l = criterion(output, current_target_sequence[i])\n",
        "                        loss += l\n",
        "\n",
        "                    # backward + update weights only if in training phase at the end of a sequence\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() / current_input_sequence.size(0)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            training_curves[phase+'_loss'].append(epoch_loss)\n",
        "\n",
        "            print(f'{phase:5} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            # deep copy the model if it's the best loss\n",
        "            # Note: We are using the train loss here to determine our best model\n",
        "            if phase == 'train' and epoch_loss < best_loss:\n",
        "              best_epoch = epoch\n",
        "              best_loss = epoch_loss\n",
        "              best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Loss: {best_loss:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, training_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h35LR8siENSg"
      },
      "outputs": [],
      "source": [
        "# We define our predict function here so that we can run some predictions in the same cell as our training!\n",
        "def predict(model, word_dictionary, word_list, input_sentence, max_length = 20):\n",
        "    output_sentence = input_sentence + \" \"\n",
        "    tensor = create_input_tensor(input_sentence, word_dictionary)\n",
        "    hidden = model.initHidden()\n",
        "    current_input_sequence = tensor.to(device)\n",
        "    input = None\n",
        "\n",
        "    for i in range(current_input_sequence.size(0)):\n",
        "        current_hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "        output, hidden = model(current_input_sequence[i], current_hidden)\n",
        "\n",
        "    topv, topi = output.topk(1)\n",
        "    topi = topi[0][0]\n",
        "    if topi ==  len(word_dictionary):\n",
        "        topv, topi = output.topk(2)\n",
        "        topi = topi[0][1]\n",
        "    word = word_list[topi]\n",
        "    output_sentence += word\n",
        "    output_sentence += \" \"\n",
        "    input = create_input_tensor(word, word_dictionary)\n",
        "\n",
        "    for i in range(len(input_sentence.split(\" \")), max_length):\n",
        "        current_hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "        current_input = input[0].to(device)\n",
        "        output, hidden = model(current_input, current_hidden)\n",
        "        topv, topi = output.topk(1)\n",
        "        topi = topi[0][0]\n",
        "        if topi == len(word_dictionary):\n",
        "            # print(\"Hit the EOS\")\n",
        "            break\n",
        "        word = word_list[topi]\n",
        "        output_sentence += word\n",
        "        output_sentence += \" \"\n",
        "        input = create_input_tensor(word, word_dictionary)\n",
        "    return output_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_2AnqgRB2x3",
        "outputId": "40827780-0ceb-4820-b370-ecaa1c7fe8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2676\n",
            "music >>> 0\n",
            "is >>> 1\n",
            "the >>> 2\n",
            "universal >>> 3\n",
            "language >>> 4\n",
            "if >>> 5\n",
            "you >>> 6\n",
            "are >>> 7\n",
            "tied >>> 8\n",
            "up >>> 9\n",
            "now >>> 10\n"
          ]
        }
      ],
      "source": [
        "sample = 10\n",
        "print(len(english_dictionary))\n",
        "for i, (k, v) in enumerate(english_dictionary.items()):\n",
        "    print(k,\">>>\",v)\n",
        "    if i == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx4m_TcPB2x3",
        "outputId": "85cf7b73-bba4-4a5e-e06f-ae999a033519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 1, 2677])\n",
            "torch.Size([5, 1, 2677])\n",
            "torch.Size([4, 1, 2677])\n",
            "torch.Size([4, 1, 2677])\n",
            "torch.Size([7, 1, 2677])\n",
            "torch.Size([7, 1, 2677])\n"
          ]
        }
      ],
      "source": [
        "print(dataloaders[\"train\"][0][1].size())\n",
        "print(dataloaders[\"train\"][0][1].size())\n",
        "print(dataloaders[\"test\"][0][0].size())\n",
        "print(dataloaders[\"test\"][0][1].size())\n",
        "print(dataloaders[\"val\"][0][0].size())\n",
        "print(dataloaders[\"val\"][0][1].size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diwgt3Fix2zg"
      },
      "source": [
        "### Visualizing Results\n",
        "\n",
        "Take a look at the training curves - does your model overfit to your training data? If so, why do you think that may be? Enter your explanation in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejU7W0o5wEIk"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEWXj81iCrvE"
      },
      "outputs": [],
      "source": [
        "def plot_training_curves(training_curves,\n",
        "                         phases=['train', 'val', 'test'],\n",
        "                         metrics=['loss']):\n",
        "    epochs = list(range(len(training_curves['train_loss'])))\n",
        "    for metric in metrics:\n",
        "        plt.figure()\n",
        "        plt.title(f'Training curves - {metric}')\n",
        "        for phase in phases:\n",
        "            key = phase+'_'+metric\n",
        "            if key in training_curves:\n",
        "                plt.plot(epochs, training_curves[key])\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(labels=phases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgaR32cNCxUC"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}