{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abrange/mit-deeplearning/blob/main/module_5/Deep_Learning_Mastering_Neural_Networks_Module_5_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZVJ8gUn1RR9"
   },
   "source": [
    "# Deep Learning: Mastering Neural Networks - Module 5 Assignment: LSTM Sentence Completion\n",
    "\n",
    "Now that we have a framework for working with sequential data in PyTorch - we would like to improve our sentence completion model by introducing a more sophisticated dataset encoding and neural network architecture.\n",
    "\n",
    "In this assignment, we would like you to implement an LSTM model that contains 2 hidden layers and completes sentences at a word level encoding instead of character. We will provide code for cleaning and preparing the data as well as some helper functions so that you can complete the task.\n",
    "\n",
    "Note: This LSTM can take a long time to train. Try using a small number of epochs or a small dataset(~10 samples) to verify your network can train properly before using the full dataset and a larger number of Epochs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cu4mucQioGZA"
   },
   "source": [
    "## Dataset and Encoding\n",
    "\n",
    "We will use the same dataset as the last notebook, however we will now use the spanish sentences as the targets for our sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yrzZRul42zJs"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FoRgk40z3kJk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-27 22:30:33--  https://www.manythings.org/anki/spa-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5453910 (5.2M) [application/zip]\n",
      "Saving to: ‘spa-eng.zip’\n",
      "\n",
      "spa-eng.zip         100%[===================>]   5.20M  2.07MB/s    in 2.5s    \n",
      "\n",
      "2025-07-27 22:30:36 (2.07 MB/s) - ‘spa-eng.zip’ saved [5453910/5453910]\n",
      "\n",
      "Archive:  spa-eng.zip\n",
      "  inflating: _about.txt              \n",
      "  inflating: spa.txt                 \n",
      "_about.txt\n",
      "Deep_Learning_Mastering_Neural_Networks_Module_5_Assignment.ipynb\n",
      "Deep_Learning_Mastering_Neural_Networks_Module_5_Introduction_to_RNNs_and_NLP_in_PyTorch.ipynb\n",
      "spa-eng.zip\n",
      "spa.txt\n"
     ]
    }
   ],
   "source": [
    "# Here we download and unzip the text file that contains all of our translated phrases\n",
    "!rm spa-eng.zip _about.txt spa.txt\n",
    "!wget https://www.manythings.org/anki/spa-eng.zip\n",
    "!unzip spa-eng.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "66lR-Qyl8BYs"
   },
   "outputs": [],
   "source": [
    "# Helper functions combined from PyTorch tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "# This is important because we want all words to be formatted the same similar\n",
    "# to our image normalization\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!'?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def parse_data(filename):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    # Throw out the attribution as it is not a part of the data\n",
    "    pairs = [[pair[0], pair[1]] for pair in pairs]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ti0WFHKX9-k-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English sentences: 142511\n"
     ]
    }
   ],
   "source": [
    "pairs = parse_data(\"spa.txt\")\n",
    "# We only want the english sentences because we aren't translating\n",
    "english_sentences = [pair[0] for pair in pairs]\n",
    "# Shuffle our dataset\n",
    "random.shuffle(english_sentences)\n",
    "print(\"Number of English sentences:\", len(english_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BngUGRRE-8P0"
   },
   "outputs": [],
   "source": [
    "# Since we already shuffled our dataset, grab a random sampling of sentences for our train, val, and test\n",
    "# Here we are using a small number of Sentences to ease training time. Feel free to use more\n",
    "train_sentences = english_sentences[:1000]\n",
    "val_sentences = english_sentences[1000:2000]\n",
    "test_sentences = english_sentences[2000:3000]\n",
    "\n",
    "# Using this function we will create a dictionary to use for our one hot encoding vectors\n",
    "def add_words_to_dict(word_dictionary, word_list, sentences):\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word in word_dictionary:\n",
    "                continue\n",
    "            else:\n",
    "                word_list.append(word)\n",
    "                word_dictionary[word] = len(word_list)-1\n",
    "\n",
    "english_dictionary = {}\n",
    "english_list = []\n",
    "add_words_to_dict(english_dictionary, english_list, train_sentences)\n",
    "add_words_to_dict(english_dictionary, english_list, val_sentences)\n",
    "add_words_to_dict(english_dictionary, english_list, test_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pszv87RStebQ"
   },
   "source": [
    "### Encoding\n",
    "\n",
    "We will encode our sequences in a very similar format to the previous tasks. However, our one-hot encoding vectors will encode over a dictionary of words instead of specific characters. This will result in a larger one hot encoding vector but a shorter overall sequence length for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3MLDsB9KSYvQ"
   },
   "outputs": [],
   "source": [
    "# Now make our training samples:\n",
    "def create_input_tensor(sentence, word_dictionary):\n",
    "    words = sentence.split(\" \")\n",
    "    tensor = torch.zeros(len(words), 1, len(word_dictionary)+1)\n",
    "    for idx in range(len(words)):\n",
    "        word = words[idx]\n",
    "        tensor[idx][0][word_dictionary[word]] = 1\n",
    "    return tensor\n",
    "\n",
    "def create_target_tensor(sentence, word_dictionary):\n",
    "    words = sentence.split(\" \")\n",
    "    tensor = torch.zeros(len(words), 1, len(word_dictionary)+1)\n",
    "    for idx in range(1, len(words)):\n",
    "        word = words[idx]\n",
    "        if word not in word_dictionary:\n",
    "            print(\"Error: This word is not in our dataset - using a zeros tensor\")\n",
    "            continue\n",
    "        tensor[idx-1][0][word_dictionary[word]] = 1\n",
    "    tensor[len(words)-1][0][len(word_dictionary)] = 1 # EOS\n",
    "    return tensor\n",
    "\n",
    "\n",
    "train_tensors = [(create_input_tensor(sentence, english_dictionary), create_target_tensor(sentence, english_dictionary)) for sentence in train_sentences]\n",
    "val_tensors = [(create_input_tensor(sentence, english_dictionary), create_target_tensor(sentence, english_dictionary)) for sentence in val_sentences]\n",
    "test_tensors = [(create_input_tensor(sentence, english_dictionary), create_target_tensor(sentence, english_dictionary)) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AnIg_qVDQhy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code helps visualize which words represent an input_tensor and its corresponding target_tensor!\n",
      "i'm leaving \n",
      "leaving <EOS>\n",
      "let go of the rope \n",
      "go of the rope <EOS>\n",
      "we were astonished when we saw their wounds \n",
      "were astonished when we saw their wounds <EOS>\n",
      "i have nothing to do with that case \n",
      "have nothing to do with that case <EOS>\n",
      "we still have more time \n",
      "still have more time <EOS>\n",
      "where do you use french \n",
      "do you use french <EOS>\n"
     ]
    }
   ],
   "source": [
    "def tensor_to_sentence(word_list, tensor):\n",
    "    sentence = \"\"\n",
    "    for i in range(tensor.size(0)):\n",
    "        topv, topi = tensor[i].topk(1)\n",
    "        if topi[0][0] == len(word_list):\n",
    "            sentence += \"<EOS>\"\n",
    "            break\n",
    "        sentence += word_list[topi[0][0]]\n",
    "        sentence += \" \"\n",
    "    return sentence\n",
    "\n",
    "print(\"This code helps visualize which words represent an input_tensor and its corresponding target_tensor!\")\n",
    "examples_to_show = 6\n",
    "count = 1\n",
    "for input, target in train_tensors:\n",
    "    print(tensor_to_sentence(english_list, input))\n",
    "    print(tensor_to_sentence(english_list, target))\n",
    "    count +=1\n",
    "    if count > examples_to_show:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ohr0XRAnl1Xx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm leaving [encode as] tensor([[[1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.,  ..., 0., 0., 0.]]])\n",
      "let go of the rope [encode as] tensor([[[0., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "we were astonished when we saw their wounds [encode as] tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "i have nothing to do with that case [encode as] tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "we still have more time [encode as] tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "where do you use french [encode as] tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a few sentence encodings, to see what those look like:\n",
    "for i in range(6):\n",
    "    print(train_sentences[i], \"[encode as]\", train_tensors[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3SjmOheE6bcn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sizes = {'train': 1000, 'val': 1000, 'test': 1000}\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {'train': train_tensors,\n",
    "               'val': val_tensors,\n",
    "               'test': test_tensors}\n",
    "\n",
    "dataset_sizes = {'train': len(train_tensors),\n",
    "                 'val': len(val_tensors),\n",
    "                 'test': len(test_tensors)}\n",
    "print(f'dataset_sizes = {dataset_sizes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKtltT2-oUuo"
   },
   "source": [
    "### LSTM Definition\n",
    "\n",
    "Fill in your model in this section - a skeleton has been provided!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "bYLj4Qvzxldp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def initHidden(self):\n",
    "        # We need two hidden layers because of our two layered lstm!\n",
    "        # Your model should be able to use this implementation of initHidden()\n",
    "        return (torch.zeros(2, self.hidden_size).to(device), torch.zeros(2, self.hidden_size).to(device))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "In this assignment, we would like you to implement an LSTM model that contains 2 hidden layers and completes sentences at a word level encoding instead of character.\n",
    "We will provide code for cleaning and preparing the data as well as some helper functions so that you can complete the task.\n",
    "\"\"\"\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.num_layers = 2\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=self.num_layers)\n",
    "        \n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "    \n",
    "        output = self.fc(output)\n",
    "\n",
    "    def initHidden(self):\n",
    "        # We need two hidden layers because of our two layered lstm!\n",
    "        # Your model should be able to use this implementation of initHidden()\n",
    "        return (torch.zeros(2, self.hidden_size).to(device), torch.zeros(2, self.hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "DoQcZxUF-QBY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.LSTM"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HRR_nA2ofl8t"
   },
   "outputs": [],
   "source": [
    "def train_lstm(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
    "    best_loss = np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Each epoch has a training, validation, and test phase\n",
    "    phases = ['train', 'val', 'test']\n",
    "\n",
    "    # Keep track of how loss evolves during training\n",
    "    training_curves = {}\n",
    "    for phase in phases:\n",
    "        training_curves[phase+'_loss'] = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data\n",
    "            for input_sequence, target_sequence in dataloaders[phase]:\n",
    "                # Now Iterate through each sequence here:\n",
    "\n",
    "                hidden = model.initHidden() # Start with a fresh hidden state\n",
    "\n",
    "                current_input_sequence = input_sequence.to(device)\n",
    "                current_target_sequence = target_sequence.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    loss = 0\n",
    "                    # Make a prediction for each element in the sequence,\n",
    "                    # keeping track of the hidden state along the way\n",
    "                    for i in range(current_input_sequence.size(0)):\n",
    "                        # Need to be clever with how we transfer our hidden layers to the device\n",
    "                        current_hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "                        output, hidden = model(current_input_sequence[i], current_hidden)\n",
    "                        l = criterion(output, current_target_sequence[i])\n",
    "                        loss += l\n",
    "\n",
    "                    # backward + update weights only if in training phase at the end of a sequence\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() / current_input_sequence.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            training_curves[phase+'_loss'].append(epoch_loss)\n",
    "\n",
    "            print(f'{phase:5} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # deep copy the model if it's the best loss\n",
    "            # Note: We are using the train loss here to determine our best model\n",
    "            if phase == 'train' and epoch_loss < best_loss:\n",
    "              best_epoch = epoch\n",
    "              best_loss = epoch_loss\n",
    "              best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Loss: {best_loss:4f} at epoch {best_epoch}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, training_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "h35LR8siENSg"
   },
   "outputs": [],
   "source": [
    "# We define our predict function here so that we can run some predictions in the same cell as our training!\n",
    "def predict(model, word_dictionary, word_list, input_sentence, max_length = 20):\n",
    "    output_sentence = input_sentence + \" \"\n",
    "    tensor = create_input_tensor(input_sentence, word_dictionary)\n",
    "    hidden = model.initHidden()\n",
    "    current_input_sequence = tensor.to(device)\n",
    "    input = None\n",
    "\n",
    "    for i in range(current_input_sequence.size(0)):\n",
    "        current_hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "        output, hidden = model(current_input_sequence[i], current_hidden)\n",
    "\n",
    "    topv, topi = output.topk(1)\n",
    "    topi = topi[0][0]\n",
    "    if topi ==  len(word_dictionary):\n",
    "        topv, topi = output.topk(2)\n",
    "        topi = topi[0][1]\n",
    "    word = word_list[topi]\n",
    "    output_sentence += word\n",
    "    output_sentence += \" \"\n",
    "    input = create_input_tensor(word, word_dictionary)\n",
    "\n",
    "    for i in range(len(input_sentence.split(\" \")), max_length):\n",
    "        current_hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "        current_input = input[0].to(device)\n",
    "        output, hidden = model(current_input, current_hidden)\n",
    "        topv, topi = output.topk(1)\n",
    "        topi = topi[0][0]\n",
    "        if topi == len(word_dictionary):\n",
    "            # print(\"Hit the EOS\")\n",
    "            break\n",
    "        word = word_list[topi]\n",
    "        output_sentence += word\n",
    "        output_sentence += \" \"\n",
    "        input = create_input_tensor(word, word_dictionary)\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2757\n",
      "i'm >>> 0\n",
      "leaving >>> 1\n",
      "let >>> 2\n",
      "go >>> 3\n",
      "of >>> 4\n",
      "the >>> 5\n",
      "rope >>> 6\n",
      "we >>> 7\n",
      "were >>> 8\n",
      "astonished >>> 9\n",
      "when >>> 10\n"
     ]
    }
   ],
   "source": [
    "sample = 10\n",
    "print(len(english_dictionary))\n",
    "for i, (k, v) in enumerate(english_dictionary.items()):\n",
    "    print(k,\">>>\",v)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2758])\n",
      "torch.Size([2, 1, 2758])\n",
      "torch.Size([8, 1, 2758])\n",
      "torch.Size([8, 1, 2758])\n",
      "torch.Size([3, 1, 2758])\n",
      "torch.Size([3, 1, 2758])\n"
     ]
    }
   ],
   "source": [
    "print(dataloaders[\"train\"][0][1].size())\n",
    "print(dataloaders[\"train\"][0][1].size())\n",
    "print(dataloaders[\"test\"][0][0].size())\n",
    "print(dataloaders[\"test\"][0][1].size())\n",
    "print(dataloaders[\"val\"][0][0].size())\n",
    "print(dataloaders[\"val\"][0][1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "xzqzmUwL8Chj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 2757\n",
      "dataset_sizes {'train': 1000, 'val': 1000, 'test': 1000}\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\u001b[32m     18\u001b[39m scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=\u001b[32m0.95\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m lstm_best_rnn, training_curves = \u001b[43mtrain_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(predict(lstm_best_rnn, english_dictionary, english_list, \u001b[33m\"\u001b[39m\u001b[33mwhat is\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(predict(lstm_best_rnn, english_dictionary, english_list, \u001b[33m\"\u001b[39m\u001b[33mmy name\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_lstm\u001b[39m\u001b[34m(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_input_sequence.size(\u001b[32m0\u001b[39m)):\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Need to be clever with how we transfer our hidden layers to the device\u001b[39;00m\n\u001b[32m     47\u001b[39m     current_hidden = (hidden[\u001b[32m0\u001b[39m].to(device), hidden[\u001b[32m1\u001b[39m].to(device))\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     output, hidden = model(current_input_sequence[i], current_hidden)\n\u001b[32m     49\u001b[39m     l = criterion(output, current_target_sequence[i])\n\u001b[32m     50\u001b[39m     loss += l\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# TODO: Fill in the necessary code to execute the training function\n",
    "\"\"\"\n",
    "Your Code Below:\n",
    "\"\"\"\n",
    "learning_rate = 0.001\n",
    "vocab_size = len(english_dictionary)  # Unique words in our vocabulary (input_size for one-hot)\n",
    "print(\"vocab_size\", vocab_size)\n",
    "print(\"dataset_sizes\", dataset_sizes)\n",
    "hidden_size = 256\n",
    "num_epochs = 5\n",
    "#word_rnn = RNN(num_letters, 128, num_letters).to(device)\n",
    "lstm_rnn = LSTM(input_size=vocab_size + 1, hidden_size=hidden_size, output_size=vocab_size).to(device)\n",
    "\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "lstm_best_rnn, training_curves = train_lstm(lstm_rnn, dataloaders, dataset_sizes,\n",
    "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "print(predict(lstm_best_rnn, english_dictionary, english_list, \"what is\"))\n",
    "print(predict(lstm_best_rnn, english_dictionary, english_list, \"my name\"))\n",
    "print(predict(lstm_best_rnn, english_dictionary, english_list, \"how are\"))\n",
    "print(predict(lstm_best_rnn, english_dictionary, english_list, \"hi\"))\n",
    "print(predict(lstm_best_rnn, english_dictionary, english_list, \"choose\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diwgt3Fix2zg"
   },
   "source": [
    "### Visualizing Results\n",
    "\n",
    "Take a look at the training curves - does your model overfit to your training data? If so, why do you think that may be? Enter your explanation in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejU7W0o5wEIk"
   },
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEWXj81iCrvE"
   },
   "outputs": [],
   "source": [
    "def plot_training_curves(training_curves,\n",
    "                         phases=['train', 'val', 'test'],\n",
    "                         metrics=['loss']):\n",
    "    epochs = list(range(len(training_curves['train_loss'])))\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.title(f'Training curves - {metric}')\n",
    "        for phase in phases:\n",
    "            key = phase+'_'+metric\n",
    "            if key in training_curves:\n",
    "                plt.plot(epochs, training_curves[key])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(labels=phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgaR32cNCxUC"
   },
   "outputs": [],
   "source": [
    "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
