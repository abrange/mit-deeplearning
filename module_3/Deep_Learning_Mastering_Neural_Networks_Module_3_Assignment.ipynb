{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abrange/mit-deeplearning/blob/main/module_3/Deep_Learning_Mastering_Neural_Networks_Module_3_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uCZ_UPrmRUC"
   },
   "source": [
    "# Module 3 Assignment: MNIST Classification\n",
    "\n",
    "Now that we have the full power of PyTorch at our disposal, we would like to classify a more concrete dataset. In particular, we are going to look at building a classifier for the [MNIST Handwritten Digits dataset](https://en.wikipedia.org/wiki/MNIST_database). This dataset contains tens of thousands of handwritten digits from 0-9 and is very commonly used for machine learning algorithm development. In this assignment, we will provide some basic dataloading code and would like you to build a deep neural network that is trained on the MNIST dataset! Feel free to reuse code you have written or seen before in previous notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JIIMjd6QmEz4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time, copy\n",
    "\n",
    "# device config (train our model on GPU if it is available which is much faster)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "empWeSSeSGV9"
   },
   "source": [
    "First we will load in our MNIST dataset. Pytorch provides built in functions for loading popular image datasets, MNIST is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nIxOn6d_DowR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# These transforms will be performed on every datapoint - in this example we want to transform every\n",
    "# datapoint to a Tensor datatype, and perform\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "mnist_train = torchvision.datasets.MNIST('', train=True, transform =transform, download=True)\n",
    "# We will split out train dataset into train and validation!\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
    "mnist_test = torchvision.datasets.MNIST('', train=False, transform = transform, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9X6r4BM4pdMv"
   },
   "outputs": [],
   "source": [
    "# We will create DataLoaders just like before with a batch size of 100\n",
    "batch_size = 100\n",
    "dataloaders = {'train': DataLoader(mnist_train, batch_size=batch_size),\n",
    "               'val': DataLoader(mnist_val, batch_size=batch_size),\n",
    "               'test': DataLoader(mnist_test, batch_size=batch_size)}\n",
    "\n",
    "dataset_sizes = {'train': len(mnist_train),\n",
    "                 'val': len(mnist_val),\n",
    "                 'test': len(mnist_test)}\n",
    "print(f'dataset_sizes = {dataset_sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6tl7_VrwUU7"
   },
   "outputs": [],
   "source": [
    "# Hint! In the Module 3 Introduction to Pytorch notebook, the Network\n",
    "# we created required the input data to be of shape Nx1 where N is the number of\n",
    "# features. Currently, our MNIST dataset is shape 28x28 as they are images. Use\n",
    "# this code snippet as you iterate through the datapoint in your dataset to flatten\n",
    "# them so it is size 784x1 and can be used with the models we designed previously!\n",
    "\n",
    "# This loop only iterates through the \"train\" datapoints\n",
    "# In the previous notebook\n",
    "phases = [\"train\", \"val\", \"test\"]\n",
    "for phase in phases:\n",
    "  for inputs, labels in dataloaders[phase]:\n",
    "    # This flattens every every batch to the correct size!\n",
    "    inputs = inputs.view(inputs.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bac-y0X2XymI"
   },
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "# model =\n",
    "\n",
    "# loss and optimizer\n",
    "# criterion =\n",
    "# optimizer =\n",
    "# scheduler =\n",
    "# Make sure you save the training curves along the way for visualization afterwards!\n",
    "# model, training_curves = train_model(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58D0TK6DfFs3"
   },
   "outputs": [],
   "source": [
    "# Utility functions for plotting your results!\n",
    "def plot_training_curves(training_curves,\n",
    "                         phases=['train', 'val', 'test'],\n",
    "                         metrics=['loss','acc']):\n",
    "    epochs = list(range(len(training_curves['train_loss'])))\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.title(f'Training curves - {metric}')\n",
    "        for phase in phases:\n",
    "            key = phase+'_'+metric\n",
    "            if key in training_curves:\n",
    "                plt.plot(epochs, training_curves[phase+'_'+metric])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(labels=phases)\n",
    "\n",
    "def classify_predictions(model, device, dataloader):\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    all_labels = torch.tensor([]).to(device)\n",
    "    all_scores = torch.tensor([]).to(device)\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    for inputs, labels in dataloader:\n",
    "        # Important! We need to flatten every datapoint\n",
    "        inputs = inputs.view(inputs.shape[0], -1)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = torch.softmax(model(inputs),dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        scores = outputs[:,1]\n",
    "        all_labels = torch.cat((all_labels, labels), 0)\n",
    "        all_scores = torch.cat((all_scores, scores), 0)\n",
    "        all_preds = torch.cat((all_preds, preds), 0)\n",
    "    return all_preds.detach().cpu(), all_labels.detach().cpu(), all_scores.detach().cpu()\n",
    "\n",
    "def plot_cm(model, device, dataloaders, phase='test'):\n",
    "    class_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    preds, labels, scores = classify_predictions(model, device, dataloaders[phase])\n",
    "\n",
    "    cm = metrics.confusion_matrix(labels, preds)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    ax = disp.plot().ax_\n",
    "    ax.set_title('Confusion Matrix -- counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5afSrDnfMR1"
   },
   "outputs": [],
   "source": [
    "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoi36p-jfPqL"
   },
   "outputs": [],
   "source": [
    "res = plot_cm(model, device, dataloaders, phase='test')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
