{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrange/mit-deeplearning/blob/main/module_3/Deep_Learning_Mastering_Neural_Networks_Module_3_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uCZ_UPrmRUC"
      },
      "source": [
        "# Module 3 Assignment: MNIST Classification\n",
        "\n",
        "Now that we have the full power of PyTorch at our disposal, we would like to classify a more concrete dataset. In particular, we are going to look at building a classifier for the [MNIST Handwritten Digits dataset](https://en.wikipedia.org/wiki/MNIST_database). This dataset contains tens of thousands of handwritten digits from 0-9 and is very commonly used for machine learning algorithm development. In this assignment, we will provide some basic dataloading code and would like you to build a deep neural network that is trained on the MNIST dataset! Feel free to reuse code you have written or seen before in previous notebooks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JIIMjd6QmEz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98023c9b-c327-4482-f572-94e2ee9a2204"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import time, copy\n",
        "\n",
        "# device config (train our model on GPU if it is available which is much faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "empWeSSeSGV9"
      },
      "source": [
        "First we will load in our MNIST dataset. Pytorch provides built in functions for loading popular image datasets, MNIST is one of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nIxOn6d_DowR"
      },
      "outputs": [],
      "source": [
        "# These transforms will be performed on every datapoint - in this example we want to transform every\n",
        "# datapoint to a Tensor datatype, and perform\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
        "mnist_train = torchvision.datasets.MNIST('', train=True, transform =transform, download=True)\n",
        "# We will split out train dataset into train and validation!\n",
        "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [50_000, 10_000])\n",
        "mnist_test = torchvision.datasets.MNIST('', train=False, transform = transform, download=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9X6r4BM4pdMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d96f291-d821-4248-c047-2a7794001512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_sizes = {'train': 50000, 'val': 10000, 'test': 10000}\n"
          ]
        }
      ],
      "source": [
        "# We will create DataLoaders just like before with a batch size of 100\n",
        "batch_size = 100\n",
        "dataloaders = {'train': DataLoader(mnist_train, batch_size=batch_size),\n",
        "               'val': DataLoader(mnist_val, batch_size=batch_size),\n",
        "               'test': DataLoader(mnist_test, batch_size=batch_size)}\n",
        "\n",
        "dataset_sizes = {'train': len(mnist_train),\n",
        "                 'val': len(mnist_val),\n",
        "                 'test': len(mnist_test)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "u6tl7_VrwUU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb9615b-356b-4573-d539-d48237868ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before torch.Size([100, 1, 28, 28]) after torch.Size([100, 784])\n",
            "end count train 1\n",
            "before torch.Size([100, 1, 28, 28]) after torch.Size([100, 784])\n",
            "end count val 1\n",
            "before torch.Size([100, 1, 28, 28]) after torch.Size([100, 784])\n",
            "end count test 1\n"
          ]
        }
      ],
      "source": [
        "# Hint! In the Module 3 Introduction to Pytorch notebook, the Network\n",
        "# we created required the input data to be of shape Nx1 where N is the number of\n",
        "# features. Currently, our MNIST dataset is shape 28x28 as they are images. Use\n",
        "# this code snippet as you iterate through the datapoint in your dataset to flatten\n",
        "# them so it is size 784x1 and can be used with the models we designed previously!\n",
        "\n",
        "# This loop only iterates through the \"train\" datapoints\n",
        "# In the previous notebook\n",
        "phases = [\"train\", \"val\", \"test\"]\n",
        "for phase in phases:\n",
        "  count = 0\n",
        "  for inputs, labels in dataloaders[phase]:\n",
        "    #print(inputs.shape, labels.shape)\n",
        "    count += 1\n",
        "\n",
        "    # This flattens every every batch to the correct size!\n",
        "    before = inputs.shape\n",
        "    inputs = inputs.view(inputs.shape[0],-1)\n",
        "    print(\"before\", before, \"after\", inputs.shape)\n",
        "    break\n",
        "\n",
        "  print(\"end count\", phase, count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.shape)\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6L9o_uPyY-O",
        "outputId": "d73a808d-e12a-4952-c6cb-ed0b6eaede20"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 784])\n",
            "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        ...,\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2 models as Python class for a Mulity Category Classifier that inherit from pytorch.nn.Model\n",
        "\n",
        "# Simple two-hidden-layer classification model\n",
        "class SimpleClassifier2Layer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
        "        super(SimpleClassifier2Layer, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size2, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Simple three-hidden-layer classification model\n",
        "class SimpleClassifier3Layer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n",
        "        super(SimpleClassifier3Layer, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size2, hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size3, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "sak5oVAnyyLD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device config (train our model on GPU if it is available which is much faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "FEATURE_LEN = 784\n",
        "# hyperparameters\n",
        "\n",
        "# model architecture\n",
        "input_size = FEATURE_LEN\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 64\n",
        "hidden_size3 = 64\n",
        "num_classes = 10\n",
        "\n",
        "# external training parameters\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "T5M1FJD82oJI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the models\n",
        "two_layer_model = SimpleClassifier2Layer(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n",
        "print(two_layer_model)\n",
        "\n",
        "three_layer_model = SimpleClassifier3Layer(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes).to(device)\n",
        "print(three_layer_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqyGwK_2zOXl",
        "outputId": "21f6f22a-97c7-4d3f-d1f9-3244db1e8190"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleClassifier2Layer(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "SimpleClassifier3Layer(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Each epoch has a training, validation, and test phase\n",
        "    phases = ['train', 'val', 'test']\n",
        "\n",
        "    # Keep track of how loss and accuracy evolves during training\n",
        "    training_curves = {}\n",
        "    for phase in phases:\n",
        "        training_curves[phase+'_loss'] = []\n",
        "        training_curves[phase+'_acc'] = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # This ensures all of our datapoints are flattened\n",
        "                # before feeding them to our model\n",
        "                inputs = inputs.view(inputs.shape[0],-1)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, predictions = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + update weights only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(predictions == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            training_curves[phase+'_loss'].append(epoch_loss)\n",
        "            training_curves[phase+'_acc'].append(epoch_acc)\n",
        "\n",
        "            print(f'{phase:5} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model if it's the best accuracy (based on validation)\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_epoch = epoch\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, training_curves"
      ],
      "metadata": {
        "id": "sQ3AgK2z9CEZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selection of criterion"
      ],
      "metadata": {
        "id": "1LpEEOfR9Xh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two-hidden-Layer Training\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
        "optimizer = torch.optim.Adam(two_layer_model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Train the model. We also will store the results of training to visualize\n",
        "two_layer_model, training_curves_two_layer = train_model(two_layer_model, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F311mjN19Syx",
        "outputId": "868f2c11-1bc5-449d-fc3d-155571555f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "----------\n",
            "train Loss: 0.5006 Acc: 0.8573\n",
            "val   Loss: 0.2983 Acc: 0.9120\n",
            "test  Loss: 0.2845 Acc: 0.9128\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n",
            "train Loss: 0.2651 Acc: 0.9212\n",
            "val   Loss: 0.2401 Acc: 0.9281\n",
            "test  Loss: 0.2275 Acc: 0.9276\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n",
            "train Loss: 0.2126 Acc: 0.9363\n",
            "val   Loss: 0.1979 Acc: 0.9399\n",
            "test  Loss: 0.1836 Acc: 0.9412\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n",
            "train Loss: 0.1770 Acc: 0.9465\n",
            "val   Loss: 0.1699 Acc: 0.9485\n",
            "test  Loss: 0.1543 Acc: 0.9528\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n",
            "train Loss: 0.1520 Acc: 0.9543\n",
            "val   Loss: 0.1523 Acc: 0.9537\n",
            "test  Loss: 0.1363 Acc: 0.9593\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n",
            "train Loss: 0.1341 Acc: 0.9596\n",
            "val   Loss: 0.1423 Acc: 0.9575\n",
            "test  Loss: 0.1249 Acc: 0.9617\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n",
            "train Loss: 0.1203 Acc: 0.9634\n",
            "val   Loss: 0.1347 Acc: 0.9606\n",
            "test  Loss: 0.1178 Acc: 0.9646\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n",
            "train Loss: 0.1091 Acc: 0.9670\n",
            "val   Loss: 0.1295 Acc: 0.9612\n",
            "test  Loss: 0.1125 Acc: 0.9662\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n",
            "train Loss: 0.0998 Acc: 0.9698\n",
            "val   Loss: 0.1245 Acc: 0.9630\n",
            "test  Loss: 0.1071 Acc: 0.9669\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n",
            "train Loss: 0.0918 Acc: 0.9726\n",
            "val   Loss: 0.1208 Acc: 0.9632\n",
            "test  Loss: 0.1041 Acc: 0.9685\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n",
            "train Loss: 0.0848 Acc: 0.9746\n",
            "val   Loss: 0.1183 Acc: 0.9638\n",
            "test  Loss: 0.1029 Acc: 0.9705\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n",
            "train Loss: 0.0784 Acc: 0.9767\n",
            "val   Loss: 0.1176 Acc: 0.9649\n",
            "test  Loss: 0.1028 Acc: 0.9704\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n",
            "train Loss: 0.0729 Acc: 0.9784\n",
            "val   Loss: 0.1157 Acc: 0.9659\n",
            "test  Loss: 0.1023 Acc: 0.9707\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n",
            "train Loss: 0.0681 Acc: 0.9797\n",
            "val   Loss: 0.1139 Acc: 0.9670\n",
            "test  Loss: 0.1010 Acc: 0.9702\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n",
            "train Loss: 0.0639 Acc: 0.9806\n",
            "val   Loss: 0.1132 Acc: 0.9676\n",
            "test  Loss: 0.1011 Acc: 0.9702\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n",
            "train Loss: 0.0602 Acc: 0.9818\n",
            "val   Loss: 0.1136 Acc: 0.9680\n",
            "test  Loss: 0.1020 Acc: 0.9690\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n",
            "train Loss: 0.0570 Acc: 0.9829\n",
            "val   Loss: 0.1152 Acc: 0.9685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bac-y0X2XymI"
      },
      "outputs": [],
      "source": [
        "# Your code here!\n",
        "\n",
        "# model =\n",
        "\n",
        "# loss and optimizer\n",
        "# criterion =\n",
        "# optimizer =\n",
        "# scheduler =\n",
        "# Make sure you save the training curves along the way for visualization afterwards!\n",
        "# model, training_curves = train_model(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58D0TK6DfFs3"
      },
      "outputs": [],
      "source": [
        "# Utility functions for plotting your results!\n",
        "def plot_training_curves(training_curves,\n",
        "                         phases=['train', 'val', 'test'],\n",
        "                         metrics=['loss','acc']):\n",
        "    epochs = list(range(len(training_curves['train_loss'])))\n",
        "    for metric in metrics:\n",
        "        plt.figure()\n",
        "        plt.title(f'Training curves - {metric}')\n",
        "        for phase in phases:\n",
        "            key = phase+'_'+metric\n",
        "            if key in training_curves:\n",
        "                plt.plot(epochs, training_curves[phase+'_'+metric])\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(labels=phases)\n",
        "\n",
        "def classify_predictions(model, device, dataloader):\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    all_labels = torch.tensor([]).to(device)\n",
        "    all_scores = torch.tensor([]).to(device)\n",
        "    all_preds = torch.tensor([]).to(device)\n",
        "    for inputs, labels in dataloader:\n",
        "        # Important! We need to flatten every datapoint\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = torch.softmax(model(inputs),dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        scores = outputs[:,1]\n",
        "        all_labels = torch.cat((all_labels, labels), 0)\n",
        "        all_scores = torch.cat((all_scores, scores), 0)\n",
        "        all_preds = torch.cat((all_preds, preds), 0)\n",
        "    return all_preds.detach().cpu(), all_labels.detach().cpu(), all_scores.detach().cpu()\n",
        "\n",
        "def plot_cm(model, device, dataloaders, phase='test'):\n",
        "    class_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    preds, labels, scores = classify_predictions(model, device, dataloaders[phase])\n",
        "\n",
        "    cm = metrics.confusion_matrix(labels, preds)\n",
        "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "    ax = disp.plot().ax_\n",
        "    ax.set_title('Confusion Matrix -- counts')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5afSrDnfMR1"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoi36p-jfPqL"
      },
      "outputs": [],
      "source": [
        "res = plot_cm(model, device, dataloaders, phase='test')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}